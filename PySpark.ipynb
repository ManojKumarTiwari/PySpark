{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "    - Setup to run pyspark on cloudxlab's juypter notebook\n",
    "    - Test Program\n",
    "    - Run Program using spark-submit\n",
    "    - Looking in Program 1 (ratings-counter.py)\n",
    "        - textFile() \n",
    "        - map() # Transformation\n",
    "        - countByValue() # Action\n",
    "    - Key/Value RDD\n",
    "        - reduceByKey()\n",
    "        - groupByKey()\n",
    "        - keys()\n",
    "        - values()\n",
    "        - join\n",
    "        - leftOuterJoin\n",
    "        - rightOuterJoin\n",
    "        - cogroup\n",
    "        - subtractByKey\n",
    "        - mapValues()\n",
    "        - reduceByKey() # Action\n",
    "        - collect() # Action\n",
    "        - Looking in Program 2 (friends-by-age.py)\n",
    "    - Filtering RDD\n",
    "        - filter()\n",
    "        - Looking in Program 3 (min-temperatures.py)\n",
    "    - Flatten an RDD\n",
    "        - flatMap()\n",
    "        - flatMapValues()\n",
    "        - Looking in Program 3 (word-count.py)\n",
    "    - Regular Expression (Not Spark related but useful to know)\n",
    "        - Looking in Program 4 (word-count-better.py)\n",
    "    - Sorting RDD\n",
    "        - sortByKey()\n",
    "        - Looking in Program 4 (word-count-better-sorted.py)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup to run pyspark on cloudxlab's juypter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the below link to run pyspark on cloudxlab's jupyter notebook and different versions of PySpark \n",
    "# https://cloudxlab.com/blog/running-pyspark-jupyter-notebook/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i'm following the above link to run PySpark version 2.4.x\n",
    "\n",
    "import os\n",
    "import sys\n",
    " \n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/spark2.4.3\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/local/anaconda/bin/python\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/local/anaconda/bin/python\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the entry points of Spark: SparkContext and SparkConf\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"appName\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Project Gutenberg EBook of The Adventures of Sherlock Holmes', 'by Sir Arthur Conan Doyle', '(#15 in our series by Sir Arthur Conan Doyle)', '', 'Copyright laws are changing all over the world. Be sure to check the', 'copyright laws for your country before downloading or redistributing', 'this or any other Project Gutenberg eBook.', '', 'This header should be the first thing seen when viewing this Project', 'Gutenberg file.  Please do not remove it.  Do not change or edit the']\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "# Testing the setup\n",
    "\n",
    "rdd = sc.textFile(\"/data/mr/wordcount/input/\")\n",
    "print(rdd.take(10))\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have a directory named UdemySparkCourse having all the files for the course \n",
    "# \"Taming Big Data with Apache Spark and Python - Hands On!\" from Udemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Program using spark-submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st program is ratings-counter.py\n",
    "# We run the code using the below steps\n",
    "# 1. cd UdemySparkCourse # Directory having the .py file\n",
    "# 2. spark-submit ratings-counter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking in the Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contents of ratings-counter.py\n",
    "# Note: The below code will throw error when running in jupyter notebook cause jupyter notebook has its own SparkContent\n",
    "\n",
    "from pyspark import SparkConf, SparkContext \n",
    "# Import SparkContext to create SparkContext and Import SparkConf to configure SparkContext (This will be in every spark script)\n",
    "import collections # Its just python thing\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"RatingsHistogram\")\n",
    "# setMaster(\"local\") implies running in local Machine, there are other configs to run on cluster or to use all cores of your system\n",
    "# setAppName(\"RatingsHistogram\") is good practise to see the log for our application in web UI\n",
    "sc = SparkContext(conf = conf)\n",
    "# Intialize the sc\n",
    "\n",
    "lines = sc.textFile(\"file:///home/manojtiwari11v6174/UdemySparkCourse/ml-100k/u.data\") # Create an RDD\n",
    "ratings = lines.map(lambda x: x.split()[2]) # split each line with \" \" and take the 3rd value and assign to ratings\n",
    "result = ratings.countByValue() # creates a tuple having each unique entry with the number of times is occurs\n",
    "\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key/Value RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking in Program 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contents of ratings-counter.py\n",
    "# Note: The below code will throw error when running in jupyter notebook cause jupyter notebook has its own SparkContent\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"FriendsByAge\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    age = int(fields[2])\n",
    "    numFriends = int(fields[3])\n",
    "    return (age, numFriends)\n",
    "\n",
    "lines = sc.textFile(\"file:///SparkCourse/fakefriends.csv\")\n",
    "rdd = lines.map(parseLine)\n",
    "totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])\n",
    "results = averagesByAge.collect()\n",
    "for result in results:\n",
    "    print(result)\n",
    "    \n",
    "# Then run the program using the spark-submit command like above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking in Program 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contents of ratings-counter.py\n",
    "# Note: The below code will throw error when running in jupyter notebook cause jupyter notebook has its own SparkContent\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"MinTemperatures\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    stationID = fields[0]\n",
    "    entryType = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    return (stationID, entryType, temperature)\n",
    "\n",
    "lines = sc.textFile(\"file:///home/manojtiwari11v6174/UdemySparkCourse/1800.csv\")\n",
    "parsedLines = lines.map(parseLine)\n",
    "minTemps = parsedLines.filter(lambda x: \"TMIN\" in x[1])\n",
    "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
    "minTemps = stationTemps.reduceByKey(lambda x, y: min(x,y))\n",
    "results = minTemps.collect();\n",
    "\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n",
    "    \n",
    "# Then run the code using spark-submit as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking in Program 3 (word-count.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contents of ratings-counter.py\n",
    "# Note: The below code will throw error when running in jupyter notebook cause jupyter notebook has its own SparkContent\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "input = sc.textFile(\"file:///home/manojtiwari11v6174/UdemySparkCourse/Book.txt\")\n",
    "words = input.flatMap(lambda x: x.split())\n",
    "wordCounts = words.countByValue()\n",
    "\n",
    "for word, count in wordCounts.items():\n",
    "    cleanWord = word.encode('ascii', 'ignore')\n",
    "    if (cleanWord):\n",
    "        print(cleanWord.decode() + \" \" + str(count))\n",
    "        \n",
    "# Then run the code using spark-submit as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expression (Not Spark related but useful to know)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking in Program 4 (word-count-better.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contents of ratings-counter.py\n",
    "# Note: The below code will throw error when running in jupyter notebook cause jupyter notebook has its own SparkContent\n",
    "\n",
    "import re\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "def normalizeWords(text):\n",
    "    return re.compile(r'\\W+', re.UNICODE).split(text.lower())\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "input = sc.textFile(\"file:///home/manojtiwari11v6174/UdemySparkCourse/Book.txt\")\n",
    "words = input.flatMap(normalizeWords)\n",
    "wordCounts = words.countByValue()\n",
    "\n",
    "for word, count in wordCounts.items():\n",
    "    cleanWord = word.encode('ascii', 'ignore')\n",
    "    if (cleanWord):\n",
    "        print(cleanWord.decode() + \" \" + str(count))\n",
    "\n",
    "# Then run the code using spark-submit as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking in Program 4 (word-count-better-sorted.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contents of ratings-counter.py\n",
    "# Note: The below code will throw error when running in jupyter notebook cause jupyter notebook has its own SparkContent\n",
    "\n",
    "import re\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "def normalizeWords(text):\n",
    "    return re.compile(r'\\W+', re.UNICODE).split(text.lower())\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "input = sc.textFile(\"file:///home/manojtiwari11v6174/UdemySparkCourse/Book.txt\")\n",
    "words = input.flatMap(normalizeWords)\n",
    "\n",
    "wordCounts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "wordCountsSorted = wordCounts.map(lambda x: (x[1], x[0])).sortByKey()\n",
    "results = wordCountsSorted.collect()\n",
    "\n",
    "for result in results:\n",
    "    count = str(result[0])\n",
    "    word = result[1].encode('ascii', 'ignore')\n",
    "    if (word):\n",
    "        print(word.decode() + \":\\t\\t\" + count)\n",
    "        \n",
    "# Then run the code using spark-submit as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
